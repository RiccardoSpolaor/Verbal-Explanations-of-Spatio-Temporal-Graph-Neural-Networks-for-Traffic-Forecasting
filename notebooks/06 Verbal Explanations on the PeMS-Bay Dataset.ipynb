{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Verbal Explanation of Spatial Temporal GNNs for Traffic Forecasting</h1>\n",
    "    <h2>Verbal Explanations on the PeMS-Bay Dataset</h2>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "The verbal translation occurs through a template-based approach. This method involves substituting placeholders in textual templates with the chosen content to form coherent narratives.\n",
    "\n",
    "The verbal translation consists of composing a series of paragraphs by exploiting the content extracted from the graphical explanations. The first paragraph describes the predicted event and briefly sums up its causes, while the second to last paragraphs illustrate in detail each cause leading to the event which is each cluster of the important subgraph. The paragraphs describing the causes are sorted by the time they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Loading the Data\n",
    "In this section the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'pems-bay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_pems_bay.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "_, node_ids_dict, _ = adj_matrix_structure\n",
    "\n",
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the node street and kilometrage dictionary.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'structured', 'node_locations.pkl'), 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the explained data.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train.npy'))[..., :1]\n",
    "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_train.npy'))[..., :1]\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val.npy'))[..., :1]\n",
    "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_val.npy'))[..., :1]\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))[..., :1]\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test.npy'))[..., :1]\n",
    "\n",
    "# Get the clustered data.\n",
    "x_train_clusters = np.load(os.path.join(BASE_DATA_DIR, 'clustered', 'x_train.npy'))\n",
    "x_val_clusters = np.load(os.path.join(BASE_DATA_DIR, 'clustered', 'x_val.npy'))\n",
    "x_test_clusters = np.load(os.path.join(BASE_DATA_DIR, 'clustered', 'x_test.npy'))\n",
    "\n",
    "\n",
    "# Get the time information of the explained data.\n",
    "x_train_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train_time.npy'))\n",
    "y_train_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_train_time.npy'))\n",
    "x_val_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val_time.npy'))\n",
    "y_val_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_val_time.npy'))\n",
    "x_test_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test_time.npy'))\n",
    "y_test_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are turned in km/h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import MPH_TO_KMH_FACTOR\n",
    "\n",
    "x_train = x_train * MPH_TO_KMH_FACTOR\n",
    "y_train = y_train * MPH_TO_KMH_FACTOR\n",
    "\n",
    "x_val = x_val * MPH_TO_KMH_FACTOR\n",
    "y_val = y_val * MPH_TO_KMH_FACTOR\n",
    "\n",
    "x_test = x_test * MPH_TO_KMH_FACTOR\n",
    "y_test = y_test * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Verbal Translation\n",
    "The translation is performed on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBAL_TRANSLATION_DIR = os.path.join(BASE_DATA_DIR, 'translated')\n",
    "\n",
    "os.makedirs(VERBAL_TRANSLATION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.verbal_explanations.verbal_translation import translate_dataset\n",
    "\n",
    "train_translated = translate_dataset(\n",
    "    x_train,\n",
    "    x_train_times,\n",
    "    x_train_clusters,\n",
    "    y_train,\n",
    "    y_train_times,\n",
    "    node_pos_dict,\n",
    "    node_info)\n",
    "    \n",
    "\n",
    "np.save(os.path.join(VERBAL_TRANSLATION_DIR, 'train.npy'), train_translated)\n",
    "\n",
    "val_translated = translate_dataset(\n",
    "    x_val,\n",
    "    x_val_times,\n",
    "    x_val_clusters,\n",
    "    y_val,\n",
    "    y_val_times,\n",
    "    node_pos_dict,\n",
    "    node_info)\n",
    "\n",
    "np.save(os.path.join(VERBAL_TRANSLATION_DIR, 'val.npy'), val_translated)\n",
    "\n",
    "test_translated = translate_dataset(\n",
    "    x_test,\n",
    "    x_test_times,\n",
    "    x_test_clusters,\n",
    "    y_test,\n",
    "    y_test_times,\n",
    "    node_pos_dict,\n",
    "    node_info)\n",
    "\n",
    "np.save(os.path.join(VERBAL_TRANSLATION_DIR, 'test.npy'), test_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an example of a verbal translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A congestion was forecasted on I 880 at km 60 on Wednesday, 25/05/2017, from 20:20 to 21:15, with an average speed of 88.10 km/h. The motivation was a series of congestions and free flows.\n",
      "\n",
      "An initial contributing congestion materialized on I 880 at km 59, with an average speed of 86.66 km/h, from 19:20 to 19:25.\n",
      "\n",
      "After that, a contributing free flow materialized, at 101.20 km/h, on, again, I 880 at kms 59 and 60 from 19:20 to 20:00.\n",
      "\n",
      "After that, an additional contributing free flow took place on Bayshore Freeway at km 63, occurring from 19:25 to 20:15 with an average speed of 113.20 km/h.\n",
      "\n",
      "After that, an extra contributing congestion materialized on, once again, I 880 at kms 59 and 60, occurring from 19:25 to 20:15 with an average speed of 67.00 km/h.\n",
      "\n",
      "Eventually, yet an extra contributing free flow materialized, at 98.33 km/h, on, yet again, I 880 at km 59 at 19:55.\n"
     ]
    }
   ],
   "source": [
    "print(test_translated[100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
