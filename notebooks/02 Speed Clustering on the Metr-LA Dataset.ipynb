{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the model in evaluation mode.\n",
    "spatial_temporal_gnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.spatial_temporal_gnn.prediction import predict\n",
    "\n",
    "# Get the data and the values predicted by the STGNN.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'y_train.npy'))\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'y_val.npy'))\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'y_test.npy'))\n",
    "\n",
    "# Get the time information of the train, validation and test sets.\n",
    "x_train_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'x_train_time.npy'))\n",
    "y_train_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'y_train_time.npy'))\n",
    "x_val_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'x_val_time.npy'))\n",
    "y_val_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'y_val_time.npy'))\n",
    "x_test_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'x_test_time.npy'))\n",
    "y_test_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the results in kilometers per hour.\n",
    "MPH_TO_KMH_FACTOR = 1.609344\n",
    "\n",
    "y_train = y_train * MPH_TO_KMH_FACTOR\n",
    "y_val = y_val * MPH_TO_KMH_FACTOR\n",
    "y_test = y_test * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_adjacency_distance_matrix)\n",
    "\n",
    "adj_distance_matrix = get_adjacency_distance_matrix(adj_matrix, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Adjacency Distance Matrix: (2484, 2484)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the Adjacency Distance Matrix: {adj_distance_matrix.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_temporal_distance_matrix)\n",
    "\n",
    "temporal_distance_matrix = get_temporal_distance_matrix(n_nodes, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Temporal Distance Matrix: (2484, 2484)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the Temporal Distance Matrix:',\n",
    "      f'{temporal_distance_matrix.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.1 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 6.07 Noise points ratio: 0.997\n",
      "\n",
      "eps: 0.1 min_samples: 7\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.946 Connected Cluster Dissimilarity: 12.1 Noise points ratio: 0.913\n",
      "\n",
      "eps: 0.15 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.993 Connected Cluster Dissimilarity: 8.35 Noise points ratio: 0.988\n",
      "\n",
      "eps: 0.15 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0.262 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.453 Connected Cluster Dissimilarity: 12.5 Noise points ratio: 0.344\n",
      "\n",
      "eps: 0.2 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.895 Connected Cluster Dissimilarity: 11.2 Noise points ratio: 0.824\n",
      "\n",
      "eps: 0.2 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 9.12 Noise points ratio: 0.981\n",
      "\n",
      "eps: 0.2 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 1.53 Noise points ratio: 0.999\n",
      "\n",
      "eps: 0.2 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.25 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.102 Connected Cluster Dissimilarity: 15 Noise points ratio: 0.079\n",
      "\n",
      "eps: 0.25 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.707 Connected Cluster Dissimilarity: 12.7 Noise points ratio: 0.64\n",
      "\n",
      "eps: 0.25 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.944 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.902\n",
      "\n",
      "eps: 0.25 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.984 Connected Cluster Dissimilarity: 10.9 Noise points ratio: 0.972\n",
      "\n",
      "eps: 0.25 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.996 Connected Cluster Dissimilarity: 4.59 Noise points ratio: 0.993\n",
      "\n",
      "eps: 0.25 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 0.54 Noise points ratio: 0.999\n",
      "\n",
      "eps: 0.25 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.3 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0607 Connected Cluster Dissimilarity: 15.4 Noise points ratio: 0.0381\n",
      "\n",
      "eps: 0.3 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.277 Connected Cluster Dissimilarity: 13.4 Noise points ratio: 0.204\n",
      "\n",
      "eps: 0.3 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.739 Connected Cluster Dissimilarity: 11.7 Noise points ratio: 0.636\n",
      "\n",
      "eps: 0.3 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.897 Connected Cluster Dissimilarity: 10.6 Noise points ratio: 0.818\n",
      "\n",
      "eps: 0.3 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.977 Connected Cluster Dissimilarity: 11.3 Noise points ratio: 0.957\n",
      "\n",
      "eps: 0.3 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 8.22 Noise points ratio: 0.982\n",
      "\n",
      "eps: 0.3 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 1.32 Noise points ratio: 0.998\n",
      "\n",
      "eps: 0.35 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0584 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.024\n",
      "\n",
      "eps: 0.35 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.102 Connected Cluster Dissimilarity: 15 Noise points ratio: 0.0693\n",
      "\n",
      "eps: 0.35 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.582 Connected Cluster Dissimilarity: 12.8 Noise points ratio: 0.487\n",
      "\n",
      "eps: 0.35 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.752 Connected Cluster Dissimilarity: 11.8 Noise points ratio: 0.651\n",
      "\n",
      "eps: 0.35 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.938 Connected Cluster Dissimilarity: 11.2 Noise points ratio: 0.891\n",
      "\n",
      "eps: 0.35 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.967 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.939\n",
      "\n",
      "eps: 0.35 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 8.11 Noise points ratio: 0.982\n",
      "\n",
      "eps: 0.4 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0674 Connected Cluster Dissimilarity: 15.8 Noise points ratio: 0.0164\n",
      "\n",
      "eps: 0.4 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0773 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.0366\n",
      "\n",
      "eps: 0.4 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.463 Connected Cluster Dissimilarity: 13.3 Noise points ratio: 0.341\n",
      "\n",
      "eps: 0.4 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.549 Connected Cluster Dissimilarity: 12.6 Noise points ratio: 0.437\n",
      "\n",
      "eps: 0.4 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.776 Connected Cluster Dissimilarity: 11.4 Noise points ratio: 0.662\n",
      "\n",
      "eps: 0.4 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.893 Connected Cluster Dissimilarity: 10.8 Noise points ratio: 0.809\n",
      "\n",
      "eps: 0.4 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.958 Connected Cluster Dissimilarity: 10.8 Noise points ratio: 0.919\n",
      "\n",
      "eps: 0.45 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0848 Connected Cluster Dissimilarity: 16 Noise points ratio: 0.0106\n",
      "\n",
      "eps: 0.45 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0846 Connected Cluster Dissimilarity: 16 Noise points ratio: 0.0242\n",
      "\n",
      "eps: 0.45 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.403 Connected Cluster Dissimilarity: 13.8 Noise points ratio: 0.268\n",
      "\n",
      "eps: 0.45 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.467 Connected Cluster Dissimilarity: 13.2 Noise points ratio: 0.347\n",
      "\n",
      "eps: 0.45 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.625 Connected Cluster Dissimilarity: 12.3 Noise points ratio: 0.512\n",
      "\n",
      "eps: 0.45 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.781 Connected Cluster Dissimilarity: 11.6 Noise points ratio: 0.666\n",
      "\n",
      "eps: 0.45 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.906 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.835\n",
      "\n",
      "eps: 0.5 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.11 Connected Cluster Dissimilarity: 16.4 Noise points ratio: 0.00775\n",
      "\n",
      "eps: 0.5 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.107 Connected Cluster Dissimilarity: 16.3 Noise points ratio: 0.018\n",
      "\n",
      "eps: 0.5 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.125 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.0584\n",
      "\n",
      "eps: 0.5 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.395 Connected Cluster Dissimilarity: 13.9 Noise points ratio: 0.248\n",
      "\n",
      "eps: 0.5 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.491 Connected Cluster Dissimilarity: 13 Noise points ratio: 0.359\n",
      "\n",
      "eps: 0.5 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.584 Connected Cluster Dissimilarity: 12.3 Noise points ratio: 0.449\n",
      "\n",
      "eps: 0.5 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.78 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.evaluation import apply_grid_search\n",
    "\n",
    "# Apply the grid search on a subset of the training set.\n",
    "apply_grid_search(\n",
    "    instances=y_train[:200],\n",
    "    eps_list=[.1, .15, .2, .25, .3, .35, .4, .45, .5],\n",
    "    min_samples_list=[5, 7, 10, 12, 15, 17, 20],\n",
    "    adj_distance_matrix=adj_distance_matrix,\n",
    "    temporal_distance_matrix=temporal_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best parameters based on the results of the grid search.\n",
    "\n",
    "EPS = .35 #.5\n",
    "MIN_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-Cluster Variance on the test set: 0.0919 Connected Cluster Dissimilarity on the test set: 15.2 Noise points ratio on the test set: 0.0544\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.evaluation import get_dataset_clustering_scores\n",
    "\n",
    "(avg_within_cluster_variance, avg_connected_cluster_dissimilarity,\n",
    " avg_noise_ratio) = get_dataset_clustering_scores(\n",
    "     y_test, adj_distance_matrix, temporal_distance_matrix, EPS, MIN_SAMPLES)\n",
    " \n",
    "print(\n",
    "    'Within-Cluster Variance on the test set:',\n",
    "    f'{avg_within_cluster_variance:.3g}',\n",
    "    'Connected Cluster Dissimilarity on the test set:',\n",
    "    f'{avg_connected_cluster_dissimilarity:.3g}',\n",
    "    'Noise points ratio on the test set:', f'{avg_noise_ratio:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = y_test[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Within Cluster Variance: 0.0421 Sample Connected Cluster Dissimilarity: 18 Sample Noise Ratio: 0.00966\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.clustering import get_clusters\n",
    "from src.explanation.clustering.evaluation import (\n",
    "    get_within_clusters_variance, get_connected_cluster_dissimilarity,\n",
    "    get_noise_ratio)\n",
    "\n",
    "clusters = get_clusters(sample, adj_distance_matrix, temporal_distance_matrix,\n",
    "                        eps=EPS, min_samples=MIN_SAMPLES)\n",
    "\n",
    "print(\n",
    "    'Sample Within Cluster Variance:',\n",
    "    f'{get_within_clusters_variance(sample, clusters):.3g}',\n",
    "    'Sample Connected Cluster Dissimilarity:',\n",
    "    f'{get_connected_cluster_dissimilarity(sample, clusters):.3g}',\n",
    "    'Sample Noise Ratio:', f'{get_noise_ratio(sample, clusters):.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 116\n"
     ]
    }
   ],
   "source": [
    "print('Number of clusters found:', len(np.unique(clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.analyisis import (\n",
    "    get_node_values_with_clusters_and_location_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df_with_clusters = \\\n",
    "    get_node_values_with_clusters_and_location_dataframe(\n",
    "        sample, clusters, node_pos_dict, locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cluster</th>\n",
       "      <th>speed</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773869</td>\n",
       "      <td>34.15497</td>\n",
       "      <td>-118.31829</td>\n",
       "      <td>0</td>\n",
       "      <td>106.795815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767541</td>\n",
       "      <td>34.11621</td>\n",
       "      <td>-118.23799</td>\n",
       "      <td>1</td>\n",
       "      <td>107.163132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>767542</td>\n",
       "      <td>34.11641</td>\n",
       "      <td>-118.23819</td>\n",
       "      <td>2</td>\n",
       "      <td>111.637352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717447</td>\n",
       "      <td>34.07248</td>\n",
       "      <td>-118.26772</td>\n",
       "      <td>3</td>\n",
       "      <td>84.031723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>717446</td>\n",
       "      <td>34.07142</td>\n",
       "      <td>-118.26572</td>\n",
       "      <td>52</td>\n",
       "      <td>45.019501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0    773869  34.15497 -118.31829        0  106.795815         0\n",
       "1    767541  34.11621 -118.23799        1  107.163132         0\n",
       "2    767542  34.11641 -118.23819        2  111.637352         0\n",
       "3    717447  34.07248 -118.26772        3   84.031723         0\n",
       "4    717446  34.07142 -118.26572       52   45.019501         0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df_with_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl.keplergl import KeplerGl\n",
    "\n",
    "m = KeplerGl(height=800, show_docs=False, data={'data': location_df_with_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "print('Metr-LA speed clusters on the first Monday:')\n",
    "show_kepler_map(location_df_with_clusters, None)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec23651083e434799750d0948e37ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'data':      sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0       773869  34.1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'data', 'metr-la', 'explainable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "from src.explanation.clustering.clustering import (\n",
    "    get_dataset_for_explainability)\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "(x_train_expl, y_train_expl,\n",
    " x_train_time_expl, y_train_time_expl) = get_dataset_for_explainability(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_train_time,\n",
    "    y_train_time,\n",
    "    EPS,\n",
    "    MIN_SAMPLES,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    total_samples=1_000)\n",
    "save(os.path.join(DATA_DIR, 'x_train.npy'), x_train_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_train.npy'), y_train_expl)\n",
    "save(os.path.join(DATA_DIR, 'x_train_time.npy'), x_train_time_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_train_time.npy'), y_train_time_expl)\n",
    "\n",
    "(x_val_expl, y_val_expl,\n",
    " x_val_time_expl, y_val_time_expl) = get_dataset_for_explainability(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_val_time,\n",
    "    y_val_time,\n",
    "    EPS,\n",
    "    MIN_SAMPLES,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    total_samples=200)\n",
    "save(os.path.join(DATA_DIR, 'x_val.npy'), x_val_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_val.npy'), y_val_expl)\n",
    "save(os.path.join(DATA_DIR, 'x_val_time.npy'), x_val_time_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_val_time.npy'), y_val_time_expl)\n",
    "\n",
    "(x_test_expl, y_test_expl,\n",
    " x_test_time_expl, y_test_time_expl) = get_dataset_for_explainability(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    x_test_time,\n",
    "    y_test_time,\n",
    "    EPS,\n",
    "    MIN_SAMPLES,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    total_samples=300)\n",
    "save(os.path.join(DATA_DIR, 'x_test.npy'), x_test_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_test.npy'), y_test_expl)\n",
    "save(os.path.join(DATA_DIR, 'x_test_time.npy'), x_test_time_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_test_time.npy'), y_test_time_expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset for explainability shapes: (999, 12, 207, 9) (999, 12, 207, 1)\n",
      "Validation dataset for explainability shapes: (198, 12, 207, 9) (198, 12, 207, 1)\n",
      "Test dataset for explainability shapes: (300, 12, 207, 9) (300, 12, 207, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset for explainability shapes:',\n",
    "      x_train_expl.shape, y_train_expl.shape)\n",
    "print('Validation dataset for explainability shapes:',\n",
    "      x_val_expl.shape, y_val_expl.shape)\n",
    "print('Test dataset for explainability shapes:',\n",
    "      x_test_expl.shape, y_test_expl.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
