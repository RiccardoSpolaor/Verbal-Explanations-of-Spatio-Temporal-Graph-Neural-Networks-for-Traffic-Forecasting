{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialTemporalGNN(\n",
       "  (encoder): Linear(in_features=9, out_features=64, bias=False)\n",
       "  (s_gnns): ModuleList(\n",
       "    (0): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (1): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (2): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (3): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (4): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (5): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (6): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (7): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (8): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (9): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (10): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (11): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (hidden_s_gnns): ModuleList(\n",
       "    (0): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (1): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (2): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (3): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (4): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (5): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (6): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (7): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (8): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (9): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (10): S_GNN(\n",
       "      (latent_encoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (1): Linear(in_features=64, out_features=32, bias=False)\n",
       "      )\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (grus): ModuleList(\n",
       "    (0): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (1): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (2): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (3): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (4): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (5): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (6): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (7): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (8): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (9): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (10): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (11): GRU(\n",
       "      (z_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (z_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (r_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_x_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (h_h_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoder): PositionalEncoder()\n",
       "  (transformer): Transformer(\n",
       "    (queries_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (keys_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (values_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (normalization): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (normalization_out): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (prediction_head): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the model in evaluation mode.\n",
    "spatial_temporal_gnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.spatial_temporal_gnn.prediction import predict\n",
    "\n",
    "# Get the data and the values predicted by the STGNN.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_train.npy'))\n",
    "y_train = predict(spatial_temporal_gnn, x_train, scaler, DEVICE)\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_val.npy'))\n",
    "y_val = predict(spatial_temporal_gnn, x_val, scaler, DEVICE)\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'processed', 'x_test.npy'))\n",
    "y_test = predict(spatial_temporal_gnn, x_test, scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the results in kilometers per hour.\n",
    "MPH_TO_KMH_FACTOR = 1.609344\n",
    "\n",
    "y_train = y_train * MPH_TO_KMH_FACTOR\n",
    "y_val = y_val * MPH_TO_KMH_FACTOR\n",
    "y_test = y_test * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPH_TO_KMH_FACTOR = 1.609344\n",
    "# sample = y_test[100] * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_reshaped = sample.reshape(-2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_total_nodes = sample_reshaped.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.std(adj_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_expanded = np.concatenate(\n",
    "    [np.concatenate([adj_matrix] * n_timesteps, axis=0)] * n_timesteps, axis=1)\n",
    "\n",
    "distance_adj_matrix = 1 - adj_matrix_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 0.        , 0.6090446 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 0.28256208, 0.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [1.        , 1.        , 1.        , ..., 0.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 0.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2484, 2484)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_adj_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Line-space the time steps between 0 and 1.\n",
    "linespaced_time_steps = np.linspace(0, 1, n_timesteps)\n",
    "\n",
    "# Repeat each time step for each node.\n",
    "extended_time_steps = np.repeat(linespaced_time_steps, n_nodes)\n",
    "\n",
    "# Add dummy dimension to the array.\n",
    "extended_time_steps = np.expand_dims(extended_time_steps, axis=1)\n",
    "\n",
    "distance_time_matrix = cdist(extended_time_steps, extended_time_steps, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance_time_matrix[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2484, 2484)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_time_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def get_clusters(\n",
    "    instance: np.ndarray, adj_distance_matrix: np.ndarray,\n",
    "    time_distance_matrix: np.ndarray, eps: float,\n",
    "    min_samples: int, speed_distance_weight: float = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the clusters of the given instance using the DBSCAN algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : ndarray\n",
    "        The spatial-temporal graph instance to cluster.\n",
    "    adj_distance_matrix : ndarray\n",
    "        The adjacency matrix of the nodes in the graph measured in distance\n",
    "        between 0 and 1.\n",
    "    time_distance_matrix : ndarray\n",
    "        The matrix measuring the distance between the time steps\n",
    "        of the nodes in the graph between 0 and 1.\n",
    "    eps : float\n",
    "        The maximum distance between two samples for one to be considered\n",
    "        as in the neighborhood of the other.\n",
    "    min_samples : int\n",
    "        The number of samples in a neighborhood for a point for it to be\n",
    "        considered as a core point.\n",
    "    speed_distance_weight : float, optional\n",
    "        The weight of the speed distance in the clustering process,\n",
    "        by default 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        The clusters of the given instance.\n",
    "    \"\"\"\n",
    "    n_timesteps, n_nodes, _ = instance.shape\n",
    "\n",
    "    # Reshape the instance to be a column vector.\n",
    "    instance = instance.reshape(-2, 1)\n",
    "\n",
    "    # Compute the distance matrix between the speed of the nodes in the graph.\n",
    "    speed_distance_matrix = cdist(instance, instance, 'euclidean')\n",
    "    # Normalize the distance matrix between 0 and 1.\n",
    "    speed_distance_matrix /= np.max(speed_distance_matrix)\n",
    "\n",
    "    # Compute the weighted distance matrix between the nodes in the graph\n",
    "    # in terms of speed and the spatial and temporal distances.\n",
    "    distance_matrix = speed_distance_matrix * speed_distance_weight +\\\n",
    "        adj_distance_matrix + time_distance_matrix\n",
    "\n",
    "    # Normalize the distance matrix between 0 and 1.\n",
    "    #distance_matrix /= np.max(distance_matrix)\n",
    "    # Set the distance between nodes that are not connected to the maximum.\n",
    "    distance_matrix[distance_adj_matrix == 1] = 1_000\n",
    "\n",
    "    # Compute the clusters of the given instance using the DBSCAN algorithm.\n",
    "    dbscan = DBSCAN(metric='precomputed', eps=eps, min_samples=min_samples,\n",
    "                    n_jobs=-1)\n",
    "    clusters = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "    # Add a dummy dimension to the clusters array.\n",
    "    clusters = np.expand_dims(clusters, axis=1)\n",
    "\n",
    "    # Reshape the clusters array to have the same shape as the instance.\n",
    "    clusters = clusters.reshape(n_timesteps, n_nodes, 1)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_within_clusters_variance(\n",
    "    instance: np.ndarray, clusters: np.ndarray) -> float:\n",
    "    \"\"\"Get the Within-Cluster Variance metric of the clusters\n",
    "    obtained on the given instance in terms of speed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : ndarray\n",
    "        The spatial-temporal graph instance on which the clusters\n",
    "        are evaluated.\n",
    "    clusters : ndarray\n",
    "        The clusters obtained on the given instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Within-Cluster Variance metric result.\n",
    "    \"\"\"\n",
    "    # Set the intial value of the numerator sum to 0.\n",
    "    numerator_sum = 0.\n",
    "    # Set the initial value of the total number of nodes to 0.\n",
    "    total_node_number = 0.\n",
    "\n",
    "    for c in np.unique(clusters):\n",
    "        # Get the sub-sample of the nodes in the graph that belong to the\n",
    "        # current cluster.\n",
    "        sub_sample = instance[clusters == c]\n",
    "        # Get the length of the sub-sample.\n",
    "        len_sub_sample = len(sub_sample)\n",
    "        # Update the total nominator sum.\n",
    "        numerator_sum += np.var(sub_sample) * len_sub_sample\n",
    "        # Update the total number of nodes with the length of the sub-sample.\n",
    "        total_node_number += len_sub_sample\n",
    "\n",
    "    return numerator_sum / (total_node_number * np.var(instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def are_clusters_connected(\n",
    "    clusters: np.ndarray, cluster_1: int, cluster_2: int,\n",
    "    adj_matrix: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether the given clusters are connected or not, by\n",
    "    observing whethere there are node spatially or temporally\n",
    "    connected between the two clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clusters : ndarray\n",
    "        The clustered instance.\n",
    "    cluster_1 : int\n",
    "        The ID of the first cluster.\n",
    "    cluster_2 : int\n",
    "        The ID of the second cluster.\n",
    "    adj_matrix : ndarray\n",
    "        The adjacency matrix of the nodes in the graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether the given clusters are connected or not.\n",
    "    \"\"\"\n",
    "    # Get the indices of the nodes that belong to the two clusters.\n",
    "    # indices = (list of timesteps, list of nodes).\n",
    "    indices_cluster_1 = np.where(clusters == cluster_1)[:-1]\n",
    "    indices_cluster_2 = np.where(clusters == cluster_2)[:-1]\n",
    "\n",
    "    # Zip the indices of the nodes that belong to the two clusters.\n",
    "    zip_indices_cluster_1 = zip(indices_cluster_1[0], indices_cluster_1[1])\n",
    "    zip_indices_cluster_2 = zip(indices_cluster_2[0], indices_cluster_2[1])\n",
    "\n",
    "    for indices_cluster_1 in zip_indices_cluster_1:\n",
    "        for indices_cluster_2 in zip_indices_cluster_2:\n",
    "            # Get the indices of the timestep and the nodes.\n",
    "            timestep_0, node_0 = indices_cluster_1\n",
    "            timestep_1, node_1 = indices_cluster_2\n",
    "            # Check if the nodes are spatially connected in the same timestep.\n",
    "            if timestep_0 == timestep_1 and (adj_matrix[node_0, node_1] > 0 or adj_matrix[node_1, node_0] > 0):\n",
    "                return True\n",
    "            # Check if the nodes are the same and temporally connected.\n",
    "            if node_0 == node_1 and np.abs(timestep_0 - timestep_1) == 1:\n",
    "                return True\n",
    "    # If no connection is found, return False.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_connected_cluster_dissimilarity(\n",
    "    instance: np.ndarray, clusters: np.ndarray,\n",
    "    adj_matrix: np.ndarray) -> float:\n",
    "    \"\"\"Get the Connected Cluster Dissimilarity metric of the clusters\n",
    "    obtained on the given instance in terms of speed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance : ndarray\n",
    "        The spatial-temporal graph instance on which the clusters\n",
    "        are evaluated.\n",
    "    clusters : ndarray\n",
    "        The clusters obtained on the given instance.\n",
    "    adj_matrix : ndarray\n",
    "        The adjacency matrix of the nodes in the graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Connected Cluster Dissimilarity metric result.\n",
    "    \"\"\"\n",
    "    # Get the total unique cluster IDs.\n",
    "    total_clusters = np.unique(clusters)\n",
    "\n",
    "    # Set the initial value of the denominator sum to 0.\n",
    "    denominator_sum = 0.\n",
    "    # Set the initial value of the nominator sum to 0.\n",
    "    nominator_sum = 0.\n",
    "\n",
    "    for i, c1 in enumerate(total_clusters):\n",
    "        for c2 in total_clusters[i+1:]:\n",
    "            # If the two clusters are not connected, continue the loop.\n",
    "            #if not are_clusters_connected(clusters, c1, c2, adj_matrix):\n",
    "            #    continue\n",
    "            # Get the sub-samples of the nodes in the graph that belong to the\n",
    "            # current clusters.\n",
    "            sub_sample1 = instance[clusters == c1]\n",
    "            sub_sample2 = instance[clusters == c2]\n",
    "            # Get the length of the sub-samples.\n",
    "            len_sub_sample1 = len(sub_sample1)\n",
    "            len_sub_sample2 = len(sub_sample2)\n",
    "            # Compute the square root of the product of the lengths.\n",
    "            sqrt_lens = np.sqrt(len_sub_sample1 * len_sub_sample2)\n",
    "            # Compute the absolute difference between the means.\n",
    "            abs_mean_diff = np.abs(np.mean(sub_sample1) - np.mean(sub_sample2))\n",
    "            # Update the nominator sum.\n",
    "            nominator_sum += sqrt_lens * abs_mean_diff\n",
    "            # Update the denominator sum.\n",
    "            denominator_sum += sqrt_lens\n",
    "\n",
    "    return nominator_sum / denominator_sum if denominator_sum > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-Cluster Variance: 1.0\n",
      "Connected Cluster Dissimilarity: 0\n"
     ]
    }
   ],
   "source": [
    "sample = y_test[100]\n",
    " \n",
    "clusters = get_clusters(sample, distance_adj_matrix, distance_time_matrix,\n",
    "                        eps=.1, min_samples=6)\n",
    "\n",
    "print('Within-Cluster Variance:', get_within_clusters_variance(sample, clusters))\n",
    "print('Connected Cluster Dissimilarity:', get_connected_cluster_dissimilarity(sample, clusters, adj_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.1 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 6.07 Noise points ratio: 0.997\n",
      "\n",
      "eps: 0.1 min_samples: 7\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.946 Connected Cluster Dissimilarity: 12.1 Noise points ratio: 0.913\n",
      "\n",
      "eps: 0.15 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.993 Connected Cluster Dissimilarity: 8.35 Noise points ratio: 0.988\n",
      "\n",
      "eps: 0.15 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0.262 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.453 Connected Cluster Dissimilarity: 12.5 Noise points ratio: 0.344\n",
      "\n",
      "eps: 0.2 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.895 Connected Cluster Dissimilarity: 11.2 Noise points ratio: 0.824\n",
      "\n",
      "eps: 0.2 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 9.12 Noise points ratio: 0.981\n",
      "\n",
      "eps: 0.2 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 1.53 Noise points ratio: 0.999\n",
      "\n",
      "eps: 0.2 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.25 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.102 Connected Cluster Dissimilarity: 15 Noise points ratio: 0.079\n",
      "\n",
      "eps: 0.25 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.707 Connected Cluster Dissimilarity: 12.7 Noise points ratio: 0.64\n",
      "\n",
      "eps: 0.25 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.944 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.902\n",
      "\n",
      "eps: 0.25 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.984 Connected Cluster Dissimilarity: 10.9 Noise points ratio: 0.972\n",
      "\n",
      "eps: 0.25 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.996 Connected Cluster Dissimilarity: 4.59 Noise points ratio: 0.993\n",
      "\n",
      "eps: 0.25 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 0.54 Noise points ratio: 0.999\n",
      "\n",
      "eps: 0.25 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.3 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0607 Connected Cluster Dissimilarity: 15.4 Noise points ratio: 0.0381\n",
      "\n",
      "eps: 0.3 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.277 Connected Cluster Dissimilarity: 13.4 Noise points ratio: 0.204\n",
      "\n",
      "eps: 0.3 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.739 Connected Cluster Dissimilarity: 11.7 Noise points ratio: 0.636\n",
      "\n",
      "eps: 0.3 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.897 Connected Cluster Dissimilarity: 10.6 Noise points ratio: 0.818\n",
      "\n",
      "eps: 0.3 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.977 Connected Cluster Dissimilarity: 11.3 Noise points ratio: 0.957\n",
      "\n",
      "eps: 0.3 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 8.22 Noise points ratio: 0.982\n",
      "\n",
      "eps: 0.3 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 1.32 Noise points ratio: 0.998\n",
      "\n",
      "eps: 0.35 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0584 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.024\n",
      "\n",
      "eps: 0.35 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.102 Connected Cluster Dissimilarity: 15 Noise points ratio: 0.0693\n",
      "\n",
      "eps: 0.35 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.582 Connected Cluster Dissimilarity: 12.8 Noise points ratio: 0.487\n",
      "\n",
      "eps: 0.35 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.752 Connected Cluster Dissimilarity: 11.8 Noise points ratio: 0.651\n",
      "\n",
      "eps: 0.35 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.938 Connected Cluster Dissimilarity: 11.2 Noise points ratio: 0.891\n",
      "\n",
      "eps: 0.35 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.967 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.939\n",
      "\n",
      "eps: 0.35 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 8.11 Noise points ratio: 0.982\n",
      "\n",
      "eps: 0.4 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0674 Connected Cluster Dissimilarity: 15.8 Noise points ratio: 0.0164\n",
      "\n",
      "eps: 0.4 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0773 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.0366\n",
      "\n",
      "eps: 0.4 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.463 Connected Cluster Dissimilarity: 13.3 Noise points ratio: 0.341\n",
      "\n",
      "eps: 0.4 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.549 Connected Cluster Dissimilarity: 12.6 Noise points ratio: 0.437\n",
      "\n",
      "eps: 0.4 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.776 Connected Cluster Dissimilarity: 11.4 Noise points ratio: 0.662\n",
      "\n",
      "eps: 0.4 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.893 Connected Cluster Dissimilarity: 10.8 Noise points ratio: 0.809\n",
      "\n",
      "eps: 0.4 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.958 Connected Cluster Dissimilarity: 10.8 Noise points ratio: 0.919\n",
      "\n",
      "eps: 0.45 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0848 Connected Cluster Dissimilarity: 16 Noise points ratio: 0.0106\n",
      "\n",
      "eps: 0.45 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0846 Connected Cluster Dissimilarity: 16 Noise points ratio: 0.0242\n",
      "\n",
      "eps: 0.45 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.403 Connected Cluster Dissimilarity: 13.8 Noise points ratio: 0.268\n",
      "\n",
      "eps: 0.45 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.467 Connected Cluster Dissimilarity: 13.2 Noise points ratio: 0.347\n",
      "\n",
      "eps: 0.45 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.625 Connected Cluster Dissimilarity: 12.3 Noise points ratio: 0.512\n",
      "\n",
      "eps: 0.45 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.781 Connected Cluster Dissimilarity: 11.6 Noise points ratio: 0.666\n",
      "\n",
      "eps: 0.45 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.906 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.835\n",
      "\n",
      "eps: 0.5 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.11 Connected Cluster Dissimilarity: 16.4 Noise points ratio: 0.00775\n",
      "\n",
      "eps: 0.5 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.107 Connected Cluster Dissimilarity: 16.3 Noise points ratio: 0.018\n",
      "\n",
      "eps: 0.5 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.125 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.0584\n",
      "\n",
      "eps: 0.5 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.395 Connected Cluster Dissimilarity: 13.9 Noise points ratio: 0.248\n",
      "\n",
      "eps: 0.5 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.491 Connected Cluster Dissimilarity: 13 Noise points ratio: 0.359\n",
      "\n",
      "eps: 0.5 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.584 Connected Cluster Dissimilarity: 12.3 Noise points ratio: 0.449\n",
      "\n",
      "eps: 0.5 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.78 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "parameter_grid = ParameterGrid({\n",
    "    'eps': [.1, .15, .2, .25, .3, .35, .4, .45, .5],\n",
    "    'min_samples': [5, 7, 10, 12, 15, 17, 20]\n",
    "    })\n",
    "\n",
    "for p in parameter_grid:\n",
    "    total_within_cluster_variance = 0.\n",
    "    total_connected_cluster_dissimilarity = 0.\n",
    "    total_noise_ratio = 0.\n",
    "    print('eps:', p['eps'], 'min_samples:', p['min_samples'])\n",
    "    for instance in y_train[:200]:\n",
    "        clusters = get_clusters(instance, distance_adj_matrix, distance_time_matrix,\n",
    "                                eps=p['eps'], min_samples=p['min_samples'])\n",
    "        within_cluster_variance = get_within_clusters_variance(\n",
    "            instance, clusters)\n",
    "        connected_cluster_dissimilarity = get_connected_cluster_dissimilarity(\n",
    "            instance, clusters, adj_matrix)\n",
    "        total_within_cluster_variance += within_cluster_variance\n",
    "        total_connected_cluster_dissimilarity += connected_cluster_dissimilarity\n",
    "        noise = clusters[clusters == -1]\n",
    "        total_noise_ratio += len(noise) / len(instance.flatten())\n",
    "        \n",
    "        \n",
    "\n",
    "    avg_within_cluster_variance = total_within_cluster_variance / len(y_train[:200])\n",
    "    avg_connected_cluster_dissimilarity = total_connected_cluster_dissimilarity / len(y_train[:200])\n",
    "    avg_noise_ratio = total_noise_ratio / len(y_train[:200])\n",
    "    \n",
    "    noise = clusters[clusters == -1]\n",
    "    \n",
    "    print('\\tWithin-Cluster Variance:', f'{avg_within_cluster_variance:.3g}', \n",
    "          'Connected Cluster Dissimilarity:', f'{avg_connected_cluster_dissimilarity:.3g}',\n",
    "          'Noise points ratio:', f'{avg_noise_ratio:.3g}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "'''parameter_grid = ParameterGrid({\n",
    "    'eps': [.05, .1, .13, .15, .17],\n",
    "    'min_samples': [4, 5, 6, 7, 8, 9, 10]\n",
    "    })''';\n",
    "\n",
    "\n",
    "'''parameter_grid = ParameterGrid({\n",
    "    'eps': [.5, .6, .7, .8, .9, 1.],\n",
    "    'min_samples': [5, 7, 10, 12, 15, 17, 20]\n",
    "    })''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.1 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 6.07 Noise points ratio: 0.997\n",
      "\n",
      "eps: 0.1 min_samples: 7\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.946 Connected Cluster Dissimilarity: 12.1 Noise points ratio: 0.913\n",
      "\n",
      "eps: 0.15 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.993 Connected Cluster Dissimilarity: 8.35 Noise points ratio: 0.988\n",
      "\n",
      "eps: 0.15 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0.262 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\[DRAFT] Clustering.ipynb Cell 36\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39meps:\u001b[39m\u001b[39m'\u001b[39m, p[\u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmin_samples:\u001b[39m\u001b[39m'\u001b[39m, p[\u001b[39m'\u001b[39m\u001b[39mmin_samples\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m instance \u001b[39min\u001b[39;00m y_train[:\u001b[39m200\u001b[39m]:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     clusters \u001b[39m=\u001b[39m get_clusters(instance, distance_adj_matrix, distance_time_matrix,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                             eps\u001b[39m=\u001b[39;49mp[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m], min_samples\u001b[39m=\u001b[39;49mp[\u001b[39m'\u001b[39;49m\u001b[39mmin_samples\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     within_cluster_variance \u001b[39m=\u001b[39m get_within_clusters_variance(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         instance, clusters)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     connected_cluster_dissimilarity \u001b[39m=\u001b[39m get_connected_cluster_dissimilarity(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         instance, clusters, adj_matrix)\n",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\[DRAFT] Clustering.ipynb Cell 36\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m distance_matrix[distance_adj_matrix \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1_000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Compute the clusters of the given instance using the DBSCAN algorithm.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m dbscan \u001b[39m=\u001b[39m DBSCAN(metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mprecomputed\u001b[39;49m\u001b[39m'\u001b[39;49m, eps\u001b[39m=\u001b[39;49meps, min_samples\u001b[39m=\u001b[39;49mmin_samples,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                 n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m clusters \u001b[39m=\u001b[39m dbscan\u001b[39m.\u001b[39mfit_predict(distance_matrix)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#X50sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Add a dummy dimension to the clusters array.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:301\u001b[0m, in \u001b[0;36mDBSCAN.__init__\u001b[1;34m(self, eps, min_samples, metric, metric_params, algorithm, leaf_size, p, n_jobs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDBSCAN\u001b[39;00m(ClusterMixin, BaseEstimator):\n\u001b[0;32m    166\u001b[0m     \u001b[39m\"\"\"Perform DBSCAN clustering from vector array or distance matrix.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \n\u001b[0;32m    168\u001b[0m \u001b[39m    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39m    DBSCAN(eps=3, min_samples=2)\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m         eps\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,\n\u001b[0;32m    304\u001b[0m         \u001b[39m*\u001b[39m,\n\u001b[0;32m    305\u001b[0m         min_samples\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m    306\u001b[0m         metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    307\u001b[0m         metric_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m         algorithm\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    309\u001b[0m         leaf_size\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[0;32m    310\u001b[0m         p\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    311\u001b[0m         n_jobs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    312\u001b[0m     ):\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps \u001b[39m=\u001b[39m eps\n\u001b[0;32m    314\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_samples \u001b[39m=\u001b[39m min_samples\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "\n",
    "for p in parameter_grid:\n",
    "    total_within_cluster_variance = 0.\n",
    "    total_connected_cluster_dissimilarity = 0.\n",
    "    total_noise_ratio = 0.\n",
    "    print('eps:', p['eps'], 'min_samples:', p['min_samples'])\n",
    "    for instance in y_train[:200]:\n",
    "        clusters = get_clusters(instance, distance_adj_matrix, distance_time_matrix,\n",
    "                                eps=p['eps'], min_samples=p['min_samples'])\n",
    "        within_cluster_variance = get_within_clusters_variance(\n",
    "            instance, clusters)\n",
    "        connected_cluster_dissimilarity = get_connected_cluster_dissimilarity(\n",
    "            instance, clusters, adj_matrix)\n",
    "        total_within_cluster_variance += within_cluster_variance\n",
    "        total_connected_cluster_dissimilarity += connected_cluster_dissimilarity\n",
    "        noise = clusters[clusters == -1]\n",
    "        total_noise_ratio += len(noise) / len(instance.flatten())\n",
    "        \n",
    "        \n",
    "\n",
    "    avg_within_cluster_variance = total_within_cluster_variance / len(y_train[:200])\n",
    "    avg_connected_cluster_dissimilarity = total_connected_cluster_dissimilarity / len(y_train[:200])\n",
    "    avg_noise_ratio = total_noise_ratio / len(y_train[:200])\n",
    "    \n",
    "    noise = clusters[clusters == -1]\n",
    "    \n",
    "    print('\\tWithin-Cluster Variance:', f'{avg_within_cluster_variance:.3g}', \n",
    "          'Connected Cluster Dissimilarity:', f'{avg_connected_cluster_dissimilarity:.3g}',\n",
    "          'Noise points ratio:', f'{avg_noise_ratio:.3g}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = .5\n",
    "MIN_SAMPLES = 5\n",
    "\n",
    "#EPS = .1\n",
    "#MIN_SAMPLES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation with eps: 0.5 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.033 Connected Cluster Dissimilarity: 4.43\n"
     ]
    }
   ],
   "source": [
    "total_within_cluster_variance = 0.\n",
    "total_connected_cluster_dissimilarity = 0.\n",
    "print('Test set evaluation with eps:', EPS, 'min_samples:', MIN_SAMPLES)\n",
    "for instance in y_test:\n",
    "    clusters = get_clusters(instance, distance_adj_matrix, distance_time_matrix,\n",
    "                            eps=EPS, min_samples=MIN_SAMPLES)\n",
    "    within_cluster_variance = get_within_clusters_variance(instance, clusters)\n",
    "    connected_cluster_dissimilarity = get_connected_cluster_dissimilarity(\n",
    "        instance, clusters, adj_matrix)\n",
    "    total_within_cluster_variance += within_cluster_variance\n",
    "    total_connected_cluster_dissimilarity += connected_cluster_dissimilarity\n",
    "\n",
    "avg_within_cluster_variance = total_within_cluster_variance / len(y_train)\n",
    "avg_connected_cluster_dissimilarity = total_connected_cluster_dissimilarity / len(y_train)\n",
    "print('\\tWithin-Cluster Variance:', f'{avg_within_cluster_variance:.3g}', \n",
    "        'Connected Cluster Dissimilarity:', f'{avg_connected_cluster_dissimilarity:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = y_test[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15982233576406152\n",
      "10.59636877024353\n"
     ]
    }
   ],
   "source": [
    "clusters = get_clusters(instance, distance_adj_matrix, distance_time_matrix,\n",
    "                        eps=EPS, min_samples=MIN_SAMPLES)\n",
    "\n",
    "print(get_within_clusters_variance(instance, clusters))\n",
    "print(get_connected_cluster_dissimilarity(instance, clusters, adj_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n",
      " 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      " 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      " 71 72 73 74 75 76 77 78 79 80]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(clusters.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 82\n"
     ]
    }
   ],
   "source": [
    "print('Number of clusters found:', len(np.unique(clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.concatenate((sample, clusters), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def get_node_values_with_location_dataframe(\n",
    "    node_values: np.ndarray, node_pos_dict: Dict[int, str],\n",
    "    locations_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get a pandas dataframe from a pandas dataframe of node speed values\n",
    "    and a pandas dataframe of node locations. The resulting dataframe\n",
    "    has for each timestamp the value of the metric for each node and\n",
    "    the location of the node in the form of latitude and longitude.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_values : ndarray\n",
    "        The numpy array containing the values of the cluster and speed of\n",
    "        each node for each timestamp.\n",
    "    locations_df : DataFrame\n",
    "        The dataframe containing the location of each node.\n",
    "    metric_name : str\n",
    "        The name of the metric that will be used in the resulting dataframe.\n",
    "    turn_datetimes_to_timestamp : bool\n",
    "        Whether to turn the datetimes to timestamp or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The resulting dataframe containing the values of the metric for\n",
    "        each node and the location of the node in the form of latitude\n",
    "        and longitude.\n",
    "    \"\"\"\n",
    "    nodes_information = []\n",
    "    \n",
    "    for time_idx, node_matrix in enumerate(node_values):\n",
    "        for node_idx, features in enumerate(node_matrix):\n",
    "            node_id = node_pos_dict[node_idx]\n",
    "\n",
    "            latitude = locations_df.loc[\n",
    "                locations_df['sensor_id'] == node_id].latitude.values[0]\n",
    "            longitude = locations_df.loc[\n",
    "                locations_df['sensor_id'] == node_id].longitude.values[0]\n",
    "    \n",
    "            nodes_information.append(\n",
    "                [node_id,\n",
    "                 latitude,\n",
    "                 longitude,\n",
    "                 features[1],\n",
    "                 features[0],\n",
    "                 time_idx])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'sensor_id': [n[0] for n in nodes_information],\n",
    "        'latitude': [n[1] for n in nodes_information],\n",
    "        'longitude': [n[2] for n in nodes_information],\n",
    "        'cluster': [n[3] for n in nodes_information],\n",
    "        'speed': [n[4] for n in nodes_information],\n",
    "        'datetime': [n[5] for n in nodes_information]\n",
    "    })\n",
    "    df['cluster'] = df['cluster'].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df_with_clusters = get_node_values_with_location_dataframe(\n",
    "    sample, node_pos_dict, locations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cluster</th>\n",
       "      <th>speed</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773869</td>\n",
       "      <td>34.15497</td>\n",
       "      <td>-118.31829</td>\n",
       "      <td>0</td>\n",
       "      <td>105.516815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767541</td>\n",
       "      <td>34.11621</td>\n",
       "      <td>-118.23799</td>\n",
       "      <td>1</td>\n",
       "      <td>106.308350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>767542</td>\n",
       "      <td>34.11641</td>\n",
       "      <td>-118.23819</td>\n",
       "      <td>2</td>\n",
       "      <td>110.634773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717447</td>\n",
       "      <td>34.07248</td>\n",
       "      <td>-118.26772</td>\n",
       "      <td>3</td>\n",
       "      <td>81.586983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>717446</td>\n",
       "      <td>34.07142</td>\n",
       "      <td>-118.26572</td>\n",
       "      <td>70</td>\n",
       "      <td>44.713619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0    773869  34.15497 -118.31829        0  105.516815         0\n",
       "1    767541  34.11621 -118.23799        1  106.308350         0\n",
       "2    767542  34.11641 -118.23819        2  110.634773         0\n",
       "3    717447  34.07248 -118.26772        3   81.586983         0\n",
       "4    717446  34.07142 -118.26572       70   44.713619         0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df_with_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl.keplergl import KeplerGl\n",
    "\n",
    "m = KeplerGl(height=800, show_docs=False, data={'data': location_df_with_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "print('Metr-LA speed clusters on the first Monday:')\n",
    "show_kepler_map(location_df_with_clusters, None)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deadc9a866324a96975b316934bd78b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'data':      sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0       773869  34.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_clusters_connected(clusters, c1, c2, adj_matrix):\n",
    "    cluster_nodes_0 = np.where(clusters == c1)[:-1]\n",
    "    cluster_nodes_1 = np.where(clusters == c2)[:-1]\n",
    "\n",
    "    for i in zip(cluster_nodes_0[0], cluster_nodes_0[1], cluster_nodes_1[0], cluster_nodes_1[1]):\n",
    "        # The nodes are spatially connected.\n",
    "        if adj_matrix[i[1], i[3]] > 0:\n",
    "            return True\n",
    "        # The nodes are the same ones and there is a temporal distance of 1.\n",
    "        if i[1] == i[3] and np.abs(i[0] - i[2]) == 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 2; dimension is 2 but corresponding boolean dimension is 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\riccardo\\Desktop\\Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting\\notebooks\\[DRAFT] Clustering.ipynb Cell 63\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#Y116sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m are_clusters_connected(clusters, c1, cluster_2, adj_matrix):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#Y116sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#Y116sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m sub_sample1 \u001b[39m=\u001b[39m sample[clusters \u001b[39m==\u001b[39;49m c1]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#Y116sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m sub_sample2 \u001b[39m=\u001b[39m sample[clusters \u001b[39m==\u001b[39m cluster_2]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/riccardo/Desktop/Verbal-Explanation-of-Graph-Neural-Networks-for-Traffic-Forecasting/notebooks/%5BDRAFT%5D%20Clustering.ipynb#Y116sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m len_sub_sample1 \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sub_sample1)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 2; dimension is 2 but corresponding boolean dimension is 1"
     ]
    }
   ],
   "source": [
    "total_clusters = np.unique(clusters)\n",
    "\n",
    "denominator_sum = 0.\n",
    "nominator_sum = 0.\n",
    "\n",
    "for i, c1 in enumerate(total_clusters):\n",
    "    for cluster_2 in total_clusters[i+1:]:\n",
    "        if not are_clusters_connected(clusters, c1, cluster_2, adj_matrix):\n",
    "            continue\n",
    "        sub_sample1 = sample[clusters == c1]\n",
    "        sub_sample2 = sample[clusters == cluster_2]\n",
    "        len_sub_sample1 = len(sub_sample1)\n",
    "        len_sub_sample2 = len(sub_sample2)\n",
    "        sqrt_lens = np.sqrt(len_sub_sample1 * len_sub_sample2)\n",
    "        nominator_sum += sqrt_lens * np.abs(np.mean(sub_sample1) - np.mean(sub_sample2))\n",
    "        denominator_sum += sqrt_lens\n",
    "\n",
    "nominator_sum / denominator_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 207, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def get_node_values_with_location_dataframe(\n",
    "    node_values: np.ndarray, node_pos_dict: Dict[int, str],\n",
    "    locations_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get a pandas dataframe from a pandas dataframe of node speed values\n",
    "    and a pandas dataframe of node locations. The resulting dataframe\n",
    "    has for each timestamp the value of the metric for each node and\n",
    "    the location of the node in the form of latitude and longitude.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_values : ndarray\n",
    "        The numpy array containing the values of the cluster and speed of\n",
    "        each node for each timestamp.\n",
    "    locations_df : DataFrame\n",
    "        The dataframe containing the location of each node.\n",
    "    metric_name : str\n",
    "        The name of the metric that will be used in the resulting dataframe.\n",
    "    turn_datetimes_to_timestamp : bool\n",
    "        Whether to turn the datetimes to timestamp or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The resulting dataframe containing the values of the metric for\n",
    "        each node and the location of the node in the form of latitude\n",
    "        and longitude.\n",
    "    \"\"\"\n",
    "    nodes_information = []\n",
    "    \n",
    "    for time_idx, node_matrix in enumerate(node_values):\n",
    "        for node_idx, features in enumerate(node_matrix):\n",
    "            node_id = node_pos_dict[node_idx]\n",
    "\n",
    "            latitude = locations_df.loc[\n",
    "                locations_df['sensor_id'] == node_id].latitude.values[0]\n",
    "            longitude = locations_df.loc[\n",
    "                locations_df['sensor_id'] == node_id].longitude.values[0]\n",
    "    \n",
    "            nodes_information.append(\n",
    "                [node_id,\n",
    "                 latitude,\n",
    "                 longitude,\n",
    "                 features[1],\n",
    "                 features[0],\n",
    "                 time_idx])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'sensor_id': [n[0] for n in nodes_information],\n",
    "        'latitude': [n[1] for n in nodes_information],\n",
    "        'longitude': [n[2] for n in nodes_information],\n",
    "        'cluster': [n[3] for n in nodes_information],\n",
    "        'speed': [n[4] for n in nodes_information],\n",
    "        'datetime': [n[5] for n in nodes_information]\n",
    "    })\n",
    "    df['cluster'] = df['cluster'].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df_with_clusters = get_node_values_with_location_dataframe(\n",
    "    sample_, node_pos_dict, locations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,  48,  28,  63,   4,  64,  65,  66,  67,  -1,\n",
       "         5,   6,  50,   7,   8,  14,  68,   9,  10,  69,  25,  51,  70,\n",
       "        61,  71,  72,  27,  11,  73,  12,  13,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  52,  15,  16,  18,  17,  53,  83,  84,  85,\n",
       "        32,  19,  44,  20,  86,  87,  21,  22,  23,  54,  55,  24,  88,\n",
       "        26,  89,  90,  91,  92,  29,  39,  93,  94,  30,  56,  95,  31,\n",
       "        96,  57,  97,  33,  58,  34,  35,  98,  36,  37,  99,  59,  47,\n",
       "        38,  60,  40, 100,  41, 101, 102, 103,  42, 104, 105, 106, 107,\n",
       "       108,  43, 109, 110, 111,  45, 112, 113, 114, 115,  46, 116, 117,\n",
       "        62, 118, 119, 120, 121, 122, 123,  49, 124, 125])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df_with_clusters['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location_df_with_clusters['cluster'] = location_df_with_clusters['cluster'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cluster</th>\n",
       "      <th>speed</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773869</td>\n",
       "      <td>34.15497</td>\n",
       "      <td>-118.31829</td>\n",
       "      <td>0</td>\n",
       "      <td>105.516815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767541</td>\n",
       "      <td>34.11621</td>\n",
       "      <td>-118.23799</td>\n",
       "      <td>1</td>\n",
       "      <td>106.308350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>767542</td>\n",
       "      <td>34.11641</td>\n",
       "      <td>-118.23819</td>\n",
       "      <td>2</td>\n",
       "      <td>110.634773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717447</td>\n",
       "      <td>34.07248</td>\n",
       "      <td>-118.26772</td>\n",
       "      <td>3</td>\n",
       "      <td>81.586983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>717446</td>\n",
       "      <td>34.07142</td>\n",
       "      <td>-118.26572</td>\n",
       "      <td>48</td>\n",
       "      <td>44.713619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>717592</td>\n",
       "      <td>34.14604</td>\n",
       "      <td>-118.22430</td>\n",
       "      <td>121</td>\n",
       "      <td>100.792572</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>717595</td>\n",
       "      <td>34.14163</td>\n",
       "      <td>-118.18290</td>\n",
       "      <td>122</td>\n",
       "      <td>109.369804</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>772168</td>\n",
       "      <td>34.16542</td>\n",
       "      <td>-118.47985</td>\n",
       "      <td>123</td>\n",
       "      <td>95.773941</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>718141</td>\n",
       "      <td>34.15133</td>\n",
       "      <td>-118.37456</td>\n",
       "      <td>19</td>\n",
       "      <td>106.050285</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>769373</td>\n",
       "      <td>34.10262</td>\n",
       "      <td>-118.31747</td>\n",
       "      <td>49</td>\n",
       "      <td>82.925262</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2484 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0       773869  34.15497 -118.31829        0  105.516815         0\n",
       "1       767541  34.11621 -118.23799        1  106.308350         0\n",
       "2       767542  34.11641 -118.23819        2  110.634773         0\n",
       "3       717447  34.07248 -118.26772        3   81.586983         0\n",
       "4       717446  34.07142 -118.26572       48   44.713619         0\n",
       "...        ...       ...        ...      ...         ...       ...\n",
       "2479    717592  34.14604 -118.22430      121  100.792572        11\n",
       "2480    717595  34.14163 -118.18290      122  109.369804        11\n",
       "2481    772168  34.16542 -118.47985      123   95.773941        11\n",
       "2482    718141  34.15133 -118.37456       19  106.050285        11\n",
       "2483    769373  34.10262 -118.31747       49   82.925262        11\n",
       "\n",
       "[2484 rows x 6 columns]"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df_with_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl.keplergl import KeplerGl\n",
    "\n",
    "m = KeplerGl(height=800, show_docs=False, data={'data': location_df_with_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "print('Metr-LA speed clusters on the first Monday:')\n",
    "show_kepler_map(location_df_with_clusters, None)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f7e838c9ec4c22ab5e809b17fb83b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'data':      sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0       773869  34.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = m.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.expand_dims(np.repeat(np.linspace(0, 1, n_timesteps), n_nodes), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate([np.repeat(np.linspace(0, 1, n_timesteps), n_nodes)] * n_nodes, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.repeat(adj_matrix, 12).reshape(speed_distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speed_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((n_total_nodes, n_total_nodes))\n",
    "\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        i_timestep = i // n_nodes\n",
    "        j_timestep = j // n_nodes\n",
    "        \n",
    "        i_id = i % n_nodes\n",
    "        j_id = j % n_nodes\n",
    "        \n",
    "        time_difference = abs(i_timestep - j_timestep)\n",
    "        #speed_difference = np.linalg.norm(sample_reshaped[i] - sample_reshaped[j])\n",
    "        \n",
    "        if (adj_matrix[i_id][j_id] > .5) * (time_difference <= 2):\n",
    "            speed_difference = np.linalg.norm(sample_reshaped[i] - sample_reshaped[j])\n",
    "        else:\n",
    "            speed_difference = float('inf')\n",
    "        \n",
    "        similarity_matrix[i][j] = speed_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = np.max(similarity_matrix[similarity_matrix != float('inf')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix[similarity_matrix == float('inf')] = max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 3.14331055e-03 3.24172974e-02 4.27729034e+00\n",
      " 4.36017609e+00 4.36315918e+00 5.13887024e+00 5.16516113e+00\n",
      " 5.19464874e+00 8.32704544e+01]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(similarity_matrix[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.796418782568813"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(metric='precomputed', eps=10., min_samples=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = dbscan.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.expand_dims(clusters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2484, 1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters.reshape(n_timesteps, n_nodes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 207, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = np.concatenate((sample, clusters), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 207, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def get_node_values_with_location_dataframe(\n",
    "    node_values: np.ndarray, node_pos_dict: Dict[int, str],\n",
    "    locations_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get a pandas dataframe from a pandas dataframe of node speed values\n",
    "    and a pandas dataframe of node locations. The resulting dataframe\n",
    "    has for each timestamp the value of the metric for each node and\n",
    "    the location of the node in the form of latitude and longitude.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_values : ndarray\n",
    "        The numpy array containing the values of the cluster and speed of\n",
    "        each node for each timestamp.\n",
    "    locations_df : DataFrame\n",
    "        The dataframe containing the location of each node.\n",
    "    metric_name : str\n",
    "        The name of the metric that will be used in the resulting dataframe.\n",
    "    turn_datetimes_to_timestamp : bool\n",
    "        Whether to turn the datetimes to timestamp or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The resulting dataframe containing the values of the metric for\n",
    "        each node and the location of the node in the form of latitude\n",
    "        and longitude.\n",
    "    \"\"\"\n",
    "    nodes_information = []\n",
    "    \n",
    "    for time_idx, node_matrix in enumerate(node_values):\n",
    "        for node_idx, features in enumerate(node_matrix):\n",
    "            node_id = node_pos_dict[node_idx]\n",
    "\n",
    "            latitude = locations_df.loc[\n",
    "                locations_df['sensor_id'] == node_id].latitude.values[0]\n",
    "            longitude = locations_df.loc[\n",
    "                locations_df['sensor_id'] == node_id].longitude.values[0]\n",
    "    \n",
    "            nodes_information.append(\n",
    "                [node_id,\n",
    "                 latitude,\n",
    "                 longitude,\n",
    "                 features[1],\n",
    "                 features[0],\n",
    "                 time_idx])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'sensor_id': [n[0] for n in nodes_information],\n",
    "        'latitude': [n[1] for n in nodes_information],\n",
    "        'longitude': [n[2] for n in nodes_information],\n",
    "        'cluster': [n[3] for n in nodes_information],\n",
    "        'speed': [n[4] for n in nodes_information],\n",
    "        'datetime': [n[5] for n in nodes_information]\n",
    "    })\n",
    "    df['cluster'] = df['cluster'].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df_with_clusters = get_node_values_with_location_dataframe(\n",
    "    sample_, node_pos_dict, locations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4, 13,  5,  6, 37,  7, 41,  8,  9, 10, 42, 43, 11,\n",
       "       44, 12, 14, 15, -1, 16, 45, 46, 17, 18, 47, 19, 20, 21, 48, 49, 50,\n",
       "       22, 23, 24, 25, 51, 52, 26, 27, 28, 29, 30, 53, 54, 31, 32, 38, 55,\n",
       "       33, 56, 57, 34, 58, 40, 35, 36, 59, 60, 39])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df_with_clusters['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location_df_with_clusters['cluster'] = location_df_with_clusters['cluster'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cluster</th>\n",
       "      <th>speed</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773869</td>\n",
       "      <td>34.15497</td>\n",
       "      <td>-118.31829</td>\n",
       "      <td>0</td>\n",
       "      <td>105.516815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767541</td>\n",
       "      <td>34.11621</td>\n",
       "      <td>-118.23799</td>\n",
       "      <td>1</td>\n",
       "      <td>106.308350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>767542</td>\n",
       "      <td>34.11641</td>\n",
       "      <td>-118.23819</td>\n",
       "      <td>1</td>\n",
       "      <td>110.634773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717447</td>\n",
       "      <td>34.07248</td>\n",
       "      <td>-118.26772</td>\n",
       "      <td>2</td>\n",
       "      <td>81.586983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>717446</td>\n",
       "      <td>34.07142</td>\n",
       "      <td>-118.26572</td>\n",
       "      <td>3</td>\n",
       "      <td>44.713619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>717592</td>\n",
       "      <td>34.14604</td>\n",
       "      <td>-118.22430</td>\n",
       "      <td>0</td>\n",
       "      <td>100.792572</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>717595</td>\n",
       "      <td>34.14163</td>\n",
       "      <td>-118.18290</td>\n",
       "      <td>1</td>\n",
       "      <td>109.369804</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>772168</td>\n",
       "      <td>34.16542</td>\n",
       "      <td>-118.47985</td>\n",
       "      <td>39</td>\n",
       "      <td>95.773941</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>718141</td>\n",
       "      <td>34.15133</td>\n",
       "      <td>-118.37456</td>\n",
       "      <td>2</td>\n",
       "      <td>106.050285</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>769373</td>\n",
       "      <td>34.10262</td>\n",
       "      <td>-118.31747</td>\n",
       "      <td>2</td>\n",
       "      <td>82.925262</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2484 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0       773869  34.15497 -118.31829        0  105.516815         0\n",
       "1       767541  34.11621 -118.23799        1  106.308350         0\n",
       "2       767542  34.11641 -118.23819        1  110.634773         0\n",
       "3       717447  34.07248 -118.26772        2   81.586983         0\n",
       "4       717446  34.07142 -118.26572        3   44.713619         0\n",
       "...        ...       ...        ...      ...         ...       ...\n",
       "2479    717592  34.14604 -118.22430        0  100.792572        11\n",
       "2480    717595  34.14163 -118.18290        1  109.369804        11\n",
       "2481    772168  34.16542 -118.47985       39   95.773941        11\n",
       "2482    718141  34.15133 -118.37456        2  106.050285        11\n",
       "2483    769373  34.10262 -118.31747        2   82.925262        11\n",
       "\n",
       "[2484 rows x 6 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df_with_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl.keplergl import KeplerGl\n",
    "\n",
    "m = KeplerGl(height=800, show_docs=False, data={'data': location_df_with_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from src.data.data_analysis import show_kepler_map\n",
    "\n",
    "print('Metr-LA speed clusters on the first Monday:')\n",
    "show_kepler_map(location_df_with_clusters, None)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff6162bb0494db69e2f15a2c9778c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'data':      sensor_id  latitude  longitude  cluster       speed  datetime\n",
       "0       773869  34.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = m.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
