{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.explanation.navigator.model import Navigator\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the STGNN in evaluation mode.\n",
    "spatial_temporal_gnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the data scaler.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the data and the values predicted by the STGNN.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train.npy'))\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val.npy'))\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test.npy'))\n",
    "\n",
    "# Get the time intervals.\n",
    "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test_time.npy'))\n",
    "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_processing import get_distance_matrix\n",
    "\n",
    "# Build the distance matrix between the nodes.\n",
    "distance_matrix = get_distance_matrix(locations_df, node_ids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30\n",
      "[100/100] - 462s - MAE: 1.61 - RMSE: 1.99 - MAPE: 5.5% - Average time: 4.62s              \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50\n",
      "[100/100] - 754s - MAE: 1.48 - RMSE: 1.77 - MAPE: 4.96% - Average time: 7.54s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30\n",
      "[100/100] - 460s - MAE: 1.62 - RMSE: 1.97 - MAPE: 5.4% - Average time: 4.6s               \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50\n",
      "[100/100] - 755s - MAE: 1.53 - RMSE: 1.89 - MAPE: 5.15% - Average time: 7.55s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30\n",
      "[100/100] - 616s - MAE: 1.82 - RMSE: 2.22 - MAPE: 6.31% - Average time: 6.16s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50\n",
      "[100/100] - 752s - MAE: 1.53 - RMSE: 1.87 - MAPE: 5.3% - Average time: 7.52s              \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30\n",
      "[100/100] - 553s - MAE: 1.56 - RMSE: 1.9 - MAPE: 5.24% - Average time: 5.53s              \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50\n",
      "[100/100] - 917s - MAE: 1.44 - RMSE: 1.8 - MAPE: 4.96% - Average time: 9.17s              \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30\n",
      "[100/100] - 552s - MAE: 1.71 - RMSE: 2.1 - MAPE: 5.89% - Average time: 5.52s              \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50\n",
      "[100/100] - 917s - MAE: 1.54 - RMSE: 1.94 - MAPE: 5.27% - Average time: 9.17s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 30\n",
      "[100/100] - 583s - MAE: 1.52 - RMSE: 1.86 - MAPE: 5.06% - Average time: 5.83s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 3 exploration_weight: 20 n_rollouts: 50\n",
      "[100/100] - 919s - MAE: 1.54 - RMSE: 1.93 - MAPE: 5.35% - Average time: 9.19s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 5 exploration_weight: 5 n_rollouts: 30\n",
      "[100/100] - 854s - MAE: 1.41 - RMSE: 1.77 - MAPE: 4.81% - Average time: 8.54s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 5 exploration_weight: 5 n_rollouts: 50\n",
      "[100/100] - 1428s - MAE: 1.33 - RMSE: 1.65 - MAPE: 4.44% - Average time: 14.3s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 5 exploration_weight: 10 n_rollouts: 30\n",
      "[100/100] - 851s - MAE: 1.56 - RMSE: 1.94 - MAPE: 5.31% - Average time: 8.51s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 5 exploration_weight: 10 n_rollouts: 50\n",
      "[100/100] - 1426s - MAE: 1.42 - RMSE: 1.76 - MAPE: 4.89% - Average time: 14.3s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 5 exploration_weight: 20 n_rollouts: 30\n",
      "[100/100] - 852s - MAE: 1.47 - RMSE: 1.78 - MAPE: 4.92% - Average time: 8.52s             \n",
      "\n",
      "Testing: cut_size_factor: 2 explanation_size_factor: 5 exploration_weight: 20 n_rollouts: 50\n",
      "[100/100] - 1426s - MAE: 1.39 - RMSE: 1.76 - MAPE: 4.63% - Average time: 14.3s             \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 30\n",
      "[100/100] - 584s - MAE: 2.23 - RMSE: 2.72 - MAPE: 7.46% - Average time: 5.84s             \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 2 exploration_weight: 5 n_rollouts: 50\n",
      "[100/100] - 973s - MAE: 1.94 - RMSE: 2.4 - MAPE: 6.52% - Average time: 9.73s              \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 30\n",
      "[100/100] - 585s - MAE: 2.24 - RMSE: 2.81 - MAPE: 7.52% - Average time: 5.85s             \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 2 exploration_weight: 10 n_rollouts: 50\n",
      "[100/100] - 973s - MAE: 1.99 - RMSE: 2.46 - MAPE: 6.75% - Average time: 9.73s             \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 30\n",
      "[100/100] - 585s - MAE: 2.2 - RMSE: 2.65 - MAPE: 7.33% - Average time: 5.85s              \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 2 exploration_weight: 20 n_rollouts: 50\n",
      "[100/100] - 973s - MAE: 1.98 - RMSE: 2.49 - MAPE: 6.79% - Average time: 9.73s             \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 30\n",
      "[100/100] - 832s - MAE: 2.07 - RMSE: 2.55 - MAPE: 6.83% - Average time: 8.32s             \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 3 exploration_weight: 5 n_rollouts: 50\n",
      "[100/100] - 1401s - MAE: 1.96 - RMSE: 2.38 - MAPE: 6.62% - Average time: 14s               \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 30\n",
      "[100/100] - 833s - MAE: 2.13 - RMSE: 2.67 - MAPE: 7.2% - Average time: 8.33s              \n",
      "\n",
      "Testing: cut_size_factor: 3 explanation_size_factor: 3 exploration_weight: 10 n_rollouts: 50\n",
      "[22/100] - 307s - MAE: 3.61 - RMSE: 4.63 - MAPE: 15.4% - Average time: 13.9s              \r"
     ]
    }
   ],
   "source": [
    "from src.explanation.monte_carlo.evaluation import apply_grid_search\n",
    "\n",
    "apply_grid_search(\n",
    "    x_train[::10],\n",
    "    y_train[::10],\n",
    "    distance_matrix,\n",
    "    spatial_temporal_gnn,\n",
    "    scaler,\n",
    "    n_rollouts_list=[30, 50],\n",
    "    explanation_size_factor_list=[2, 3, 5],\n",
    "    cut_size_factor_list=[2, 3],\n",
    "    exploration_weight_list=[5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 1/50\n",
      "reward: -0.1568386843706285 , mae: 6.375977993011475\n",
      "Execution 2/50\n",
      "reward: -0.1568386843706285 , mae: 6.375977993011475\n",
      "Execution 3/50\n",
      "reward: -0.17692743278257236 , mae: 5.652034759521484\n",
      "Execution 4/50\n",
      "reward: -0.17692743278257236 , mae: 5.652034759521484\n",
      "Execution 5/50\n",
      "reward: -0.17692743278257236 , mae: 5.652034759521484\n",
      "Execution 6/50\n",
      "reward: -0.17692743278257236 , mae: 5.652034759521484\n",
      "Execution 7/50\n",
      "reward: -0.17692743278257236 , mae: 5.652034759521484\n",
      "Execution 8/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 9/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 10/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 11/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 12/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 13/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 14/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 15/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 16/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 17/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 18/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 19/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 20/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 21/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 22/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 23/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 24/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 25/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 26/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 27/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 28/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 29/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 30/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 31/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 32/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 33/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 34/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 35/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 36/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 37/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 38/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 39/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 40/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 41/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 42/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 43/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 44/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 45/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 46/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 47/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 48/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 49/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "Execution 50/50\n",
      "reward: -0.514044160105184 , mae: 1.945358157157898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.monte_carlo.search import get_best_input_events_subset\n",
    "\n",
    "# Randomize list\n",
    "ls = list(zip(x_train, y_train))\n",
    "\n",
    "import random\n",
    "#random.shuffle(ls)\n",
    "\n",
    "res = []\n",
    "y_samples = []\n",
    "ls1 = [53, 224, 259, 666, 715, 770, 838, 885, 891, 947]\n",
    "\n",
    "for x_sample, y_sample in ls[1:2]:#[ls[i] for i in [53, 224, 259, 666, 715, 770, 838, 885, 891, 947]]:\n",
    "    explanation_size = int((y_sample.flatten() != 0).sum() * 2)\n",
    "    subset = get_best_input_events_subset(\n",
    "        x_sample,\n",
    "        y_sample,\n",
    "        distance_matrix,\n",
    "        spatial_temporal_gnn,\n",
    "        scaler,\n",
    "        n_rollouts=50,\n",
    "        n_top_events=explanation_size*2,\n",
    "        exploration_weight=20,\n",
    "        explanation_size=explanation_size,\n",
    "        remove_value=0.,\n",
    "        verbose=True)\n",
    "    res.append(subset)\n",
    "    y_samples.append(y_sample)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 28), (8, 28), (7, 27), (6, 27), (5, 28), (4, 27), (3, 27), (3, 28), (2, 27), (2, 28), (1, 27), (7, 177), (6, 177), (4, 177), (3, 177), (2, 177), (1, 177), (0, 2), (11, 79), (10, 79), (9, 79), (8, 79), (7, 79), (6, 79)]\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train[8].flatten().sum() / (y_train[8] != 0).flatten().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "\n",
    "input_events_subset = res[i]\n",
    "y_sample = y_samples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_events_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 69), (7, 113), (5, 73), (3, 52), (0, 69), (5, 113), (3, 73), (2, 52), (4, 113), (1, 73), (3, 113), (0, 52), (0, 73), (2, 113), (0, 113), (9, 141), (8, 141), (5, 141), (1, 141), (11, 164), (10, 164), (5, 164), (4, 164), (2, 164)]\n"
     ]
    }
   ],
   "source": [
    "print(input_events_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.events import remove_features_by_events\n",
    "\n",
    "x_subset = x_sample.copy()\n",
    "\n",
    "x_subset = remove_features_by_events(x_subset, input_events_subset)\n",
    "\n",
    "#for e in input_events_subset:\n",
    "#    x_subset[e[1], e[2], 0] = 0.\n",
    "    \n",
    "x_subset = x_subset[..., :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 207, 1)\n",
      "(12, 207, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_subset.shape)\n",
    "print(y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the input events subset with the output events.\n",
    "explained_instance = np.concatenate((x_subset, y_sample), axis=0)\n",
    "\n",
    "\n",
    "MPH_TO_KMH_FACTOR = 1.609344\n",
    "#explained_instance *= MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 207, 1)\n"
     ]
    }
   ],
   "source": [
    "print(explained_instance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.zeros_like(explained_instance)\n",
    "clusters[12:] = 1.\n",
    "clusters[explained_instance == 0.] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.clustering.analyisis import (\n",
    "    get_node_values_with_clusters_and_location_dataframe)\n",
    "\n",
    "location_df_with_clusters = \\\n",
    "    get_node_values_with_clusters_and_location_dataframe(\n",
    "        explained_instance, clusters, node_pos_dict, locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl.keplergl import KeplerGl\n",
    "\n",
    "m = KeplerGl(height=800, show_docs=False, data={'data': location_df_with_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035d4710d2c345bea247c97a88129e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'data':      sensor_id  latitude  longitude  cluster  speed  datetime\n",
       "0       773869  34.15497 â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7947202115917098 1.258304476737976\n",
      "0.3220503831806981 3.1051042079925537\n",
      "0.10994488160575627 9.095466613769531\n",
      "0.4848132338640501 2.062649965286255\n",
      "0.6544738526296934 1.5279449224472046\n",
      "0.7828435674063169 1.277394413948059\n",
      "0.391424523348731 2.5547709465026855\n",
      "0.38892625281693777 2.571181535720825\n",
      "0.5395853457235139 1.853274941444397\n",
      "0.37837483818295814 2.6428818702697754\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.monte_carlo.search import get_explanations_from_data\n",
    "\n",
    "x_explained, y_explained = get_explanations_from_data(\n",
    "    x_test[:10],\n",
    "    y_test[:10],\n",
    "    adj_matrix,\n",
    "    spatial_temporal_gnn,\n",
    "    navigator,\n",
    "    distance_matrix,\n",
    "    scaler,\n",
    "    n_rollouts=30,\n",
    "    exploration_weight=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "EXPLAINED_DATA_DIR = os.path.join(BASE_DATA_DIR, 'explained')\n",
    "os.makedirs(EXPLAINED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Save the explained data.\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test.npy'), x_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test.npy'), y_explained)\n",
    "\n",
    "# Save the explained time information of the datasets.\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test_time.npy'), x_test_time[:10])\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test_time.npy'), y_test_time[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
